from Standard.Base import all
from Standard.Base.Random import random_uuid
import Standard.Base.Errors.Illegal_Argument.Illegal_Argument
import Standard.Base.Errors.Illegal_State.Illegal_State

import Standard.Table.Data.Table.Table as In_Memory_Table
import Standard.Table.Data.Type.Value_Type.Value_Type
from Standard.Table import Aggregate_Column
from Standard.Table.Errors import all

import project.Connection.Connection.Connection
import project.Data.SQL_Query.SQL_Query
import project.Data.SQL_Statement.SQL_Statement
import project.Data.Table.Table as Database_Table
import project.Data.Update_Action.Update_Action
import project.Internal.In_Transaction.In_Transaction
import project.Internal.IR.Query.Query
import project.Internal.IR.SQL_Expression.SQL_Expression
from project.Connection.Connection import all_known_table_names
from project.Errors import all

## PRIVATE
   Creates a new database table with the provided structure and returns the name
   of the created table.
create_table_structure connection table_name structure primary_key temporary allow_existing on_problems =
    case table_name.is_nothing.not && all_known_table_names connection . contains table_name of
        True ->
            if allow_existing then table_name else Error.throw (Table_Already_Exists.Error table_name)
        False ->
            effective_table_name = resolve_effective_table_name table_name temporary
            aligned_structure = align_structure structure
            resolved_primary_key = resolve_primary_key aligned_structure primary_key
            create_table_statement = prepare_create_table_statement connection effective_table_name aligned_structure resolved_primary_key temporary on_problems
            update_result = create_table_statement.if_not_error <|
                connection.execute_update create_table_statement
            update_result.if_not_error <|
                effective_table_name

## PRIVATE
   A helper that can upload a table from any backend to a database.
   It should be run within a transaction and wrapped in `handle_upload_errors`.
internal_upload_table source_table connection table_name primary_key temporary on_problems =
    case source_table of
        _ : In_Memory_Table ->
            internal_upload_in_memory_table source_table connection table_name primary_key temporary on_problems
        _ : Database_Table ->
            internal_upload_database_table source_table connection table_name primary_key temporary on_problems
        _ ->
            Panic.throw <| Illegal_Argument.Error ("Unsupported table type: " + Meta.get_qualified_type_name source_table)

## PRIVATE
upload_in_memory_table source_table connection table_name primary_key temporary on_problems =
    Panic.recover SQL_Error <| handle_upload_errors <|
        connection.jdbc_connection.run_within_transaction <|
            internal_upload_in_memory_table source_table connection table_name primary_key temporary on_problems

## PRIVATE
   It should be run within a transaction and wrapped in `handle_upload_errors`.
internal_upload_in_memory_table source_table connection table_name primary_key temporary on_problems =
    In_Transaction.ensure_in_transaction <|
        created_table_name = create_table_structure connection table_name structure=source_table primary_key=primary_key temporary=temporary allow_existing=False on_problems=on_problems
        column_names = source_table.column_names

        ## `created_table_name.if_not_error` is used to ensure that if there are
           any dataflow errors up to this point, we want to propagate them and not
           continue. Otherwise, they could 'leak' to `Panic.rethrow` and be wrongly
           raised as panics.
        upload_status = created_table_name.if_not_error <|
            internal_translate_known_upload_errors source_table connection primary_key <|
                insert_template = make_batched_insert_template connection created_table_name column_names
                statement_setter = connection.dialect.get_statement_setter
                Panic.rethrow <| connection.jdbc_connection.batch_insert insert_template statement_setter source_table default_batch_size

        upload_status.if_not_error <|
            connection.query (SQL_Query.Table_Name created_table_name)

## PRIVATE
upload_database_table source_table connection table_name primary_key temporary on_problems =
    Panic.recover SQL_Error <| handle_upload_errors <|
        connection.jdbc_connection.run_within_transaction <|
            internal_upload_database_table source_table connection table_name primary_key temporary on_problems

## PRIVATE
   It should be run within a transaction and wrapped in `handle_upload_errors`.
internal_upload_database_table source_table connection table_name primary_key temporary on_problems =
    In_Transaction.ensure_in_transaction <|
        connection_check = if source_table.connection.jdbc_connection == connection.jdbc_connection then True else
            Error.throw (Unsupported_Database_Operation.Error "The Database table to be uploaded must be coming from the same connection as the connection on which the new table is being created. Cross-connection uploads are currently not supported. To work around this, you can first `.read` the table into memory and then upload it from memory to a different connection.")

        connection_check.if_not_error <|
            created_table_name = create_table_structure connection table_name structure=source_table primary_key=primary_key temporary=temporary allow_existing=False on_problems=on_problems
            upload_status = created_table_name.if_not_error <|
                internal_translate_known_upload_errors source_table connection primary_key <|
                    ## We need to ensure that the columns in this statement are
                       matching positionally the columns in the newly created
                       table. But we create both from the same source table, so
                       that is guaranteed.
                    copy_into_statement = connection.dialect.generate_sql <|
                        Query.Insert_From_Select created_table_name source_table.to_select_query
                    Panic.rethrow <| connection.execute_update copy_into_statement

            upload_status.if_not_error <|
                connection.query (SQL_Query.Table_Name created_table_name)

## PRIVATE
   Ensures that provided primary key columns are present in the table and that
   there are no duplicates.
resolve_primary_key structure primary_key = case primary_key of
    Nothing -> Nothing
    _ : Vector -> if primary_key.is_empty then Nothing else
        validated = primary_key.map key->
            if key.is_a Text then key else
                Error.throw (Illegal_Argument.Error "Primary key must be a vector of column names.")
        validated.if_not_error <|
            column_names = Set.from_vector (structure.map .first)
            missing_columns = (Set.from_vector primary_key).difference column_names
            if missing_columns.not_empty then Error.throw (Missing_Input_Columns.Error missing_columns.to_vector) else
                primary_key

## PRIVATE
   Inspects any `SQL_Error` thrown and replaces it with an error recipe, that is
   converted into a proper error in an outer layer.

   The special handling is needed, because computing the
   `Non_Unique_Primary_Key` error may need to perform a SQL query that must be
   run outside of the just-failed transaction.
internal_translate_known_upload_errors source_table connection primary_key ~action =
    handler caught_panic =
        error_mapper = connection.dialect.get_error_mapper
        sql_error = caught_panic.payload
        case error_mapper.is_primary_key_violation sql_error of
            True -> Panic.throw (Non_Unique_Primary_Key_Recipe.Recipe source_table primary_key caught_panic)
            False -> Panic.throw caught_panic
    Panic.catch SQL_Error action handler

## PRIVATE
handle_upload_errors ~action =
    Panic.catch Non_Unique_Primary_Key_Recipe action caught_panic->
        recipe = caught_panic.payload
        raise_duplicated_primary_key_error recipe.source_table recipe.primary_key recipe.original_panic

## PRIVATE
type Non_Unique_Primary_Key_Recipe
    ## PRIVATE
    Recipe source_table primary_key original_panic

## PRIVATE
   Creates a `Non_Unique_Primary_Key` error containing information about an
   example group violating the uniqueness constraint.
raise_duplicated_primary_key_error source_table primary_key original_panic =
    agg = source_table.aggregate [Aggregate_Column.Count]+(primary_key.map Aggregate_Column.Group_By)
    filtered = agg.filter column=0 (Filter_Condition.Greater than=1)
    materialized = filtered.read max_rows=1
    case materialized.row_count == 0 of
        ## If we couldn't find a duplicated key, we give up the translation and
           rethrow the original panic containing the SQL error. This could
           happen if the constraint violation is on some non-trivial key, like
           case insensitive.
        True -> Panic.throw original_panic
        False ->
            row = materialized.first_row.to_vector
            example_count = row.first
            example_entry = row.drop 1
            Error.throw (Non_Unique_Primary_Key.Error primary_key example_entry example_count)

## PRIVATE
align_structure : Database_Table | In_Memory_Table | Vector (Pair Text Value_Type) -> Vector (Pair Text Value_Type)
align_structure table_or_columns = case table_or_columns of
    _ : Vector -> table_or_columns.map pair->
        if pair.length != 2 then Error.throw (Illegal_Argument.Error "The structure must be an existing Table or vector of pairs.") else
            name = pair.first
            value_type = pair.second
            if name . is_a Text . not then Error.throw (Illegal_Argument.Error "Column names must be a Text. Instead, got a: "+name.to_display_text+".") else
                if value_type . is_a Value_Type . not then Error.throw (Illegal_Argument.Error "Column value types must be a Value_Type. Instead, got a: "+value_type.to_display_text+".") else
                    pair
    _ -> table_or_columns.columns.map column->
        Pair.new column.name column.value_type

## PRIVATE
   Creates a statement that will create a table with structure determined by the
   provided columns.

   The `primary_key` columns must be present in `columns`, but it is the
   responsibility of the caller to ensure that, otherwise the generated
   statement will be invalid.
prepare_create_table_statement : Connection -> Text -> Vector -> Vector Text -> Boolean -> Problem_Behavior -> SQL_Statement
prepare_create_table_statement connection table_name columns primary_key temporary on_problems =
    type_mapping = connection.dialect.get_type_mapping
    column_descriptors = columns.map pair->
        name = pair.first
        value_type = pair.second
        sql_type = type_mapping.value_type_to_sql value_type on_problems
        sql_type_text = type_mapping.sql_type_to_text sql_type
        Pair.new name sql_type_text
    connection.dialect.generate_sql <|
        Query.Create_Table table_name column_descriptors primary_key temporary

## PRIVATE
   Generates a random table name if it was nothing, if it is allowed (temporary=True).
resolve_effective_table_name table_name temporary = case table_name of
    Nothing -> if temporary then "temporary-table-"+random_uuid else
        Error.throw (Illegal_Argument.Error "A name must be provided when creating a non-temporary table.")
    _ : Text -> table_name

## PRIVATE
   The recommended batch size seems to be between 50 and 100.
   See: https://docs.oracle.com/cd/E18283_01/java.112/e16548/oraperf.htm#:~:text=batch%20sizes%20in%20the%20general%20range%20of%2050%20to%20100
default_batch_size = 100

## PRIVATE
make_batched_insert_template : Connection -> Text -> Vector (Vector Text) -> SQL_Query
make_batched_insert_template connection table_name column_names =
    # We add Nothing as placeholders, they will be replaced with the actual values later.
    pairs = column_names.map name->[name, SQL_Expression.Constant Nothing]
    query = connection.dialect.generate_sql <| Query.Insert table_name pairs
    template = query.prepare.first
    template

## PRIVATE
common_update_table source_table connection table_name update_action key_columns error_on_missing_columns =
    Panic.recover SQL_Error <| handle_upload_errors <|
        connection.jdbc_connection.run_within_transaction <|
            target_table = connection.query (SQL_Query.Table_Name table_name)
            # We catch the `Table_Not_Found` error and handle it specially, if the error was different, it will just get passed through further.
            handle_error = target_table.catch Table_Not_Found error->
                # Rethrow the error with more info.
                msg_suffix = " Use `Connection.create_table` to create a table before trying to append to it."
                new_error = error.with_changed_extra_message msg_suffix
                Error.throw new_error
            if target_table.is_error then handle_error else
                tmp_table_name = "temporary-source-table-"+random_uuid
                tmp_table = internal_upload_table source_table connection tmp_table_name key_columns temporary=True structure_only=False on_problems=Problem_Behavior.Report_Error
                tmp_table.if_not_error <|
                    resulting_table = append_to_existing_table tmp_table target_table update_action key_columns error_on_missing_columns
                    connection.drop_table tmp_table.name
                    resulting_table

## PRIVATE
append_to_existing_table source_table target_table update_action key_columns error_on_missing_columns =
    source_columns = Set.from_vector source_table.column_names
    target_columns = Set.from_vector target_table.column_names
    extra_columns = source_columns.difference target_columns
    if extra_columns.not_empty then Error.throw (Unmatched_Columns.Error extra_columns) else
        missing_columns = target_columns.difference source_columns
        if missing_columns.not_empty && error_on_missing_columns then Error.throw (Missing_Input_Columns.Error missing_columns "the source table") else
            # TODO [RW] to be finished in follow up PR
            _ = [update_action, key_columns]
            Error.throw (Illegal_State.Error "TODO: Not implemented yet.")
