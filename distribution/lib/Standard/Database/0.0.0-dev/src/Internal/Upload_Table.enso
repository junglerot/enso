from Standard.Base import all
import Standard.Base.Errors.Common.Dry_Run_Operation
import Standard.Base.Errors.Common.Forbidden_Operation
import Standard.Base.Errors.Illegal_Argument.Illegal_Argument
import Standard.Base.Errors.Illegal_State.Illegal_State
import Standard.Base.Runtime.Context

import Standard.Table.Data.Table.Table as In_Memory_Table
import Standard.Table.Internal.Problem_Builder.Problem_Builder
from Standard.Table import Aggregate_Column, Join_Kind, Value_Type
from Standard.Table.Errors import all

import project.Connection.Connection.Connection
import project.Data.Column_Constraint.Column_Constraint
import project.Data.Column_Description.Column_Description
import project.Data.SQL_Query.SQL_Query
import project.Data.SQL_Statement.SQL_Statement
import project.Data.Table.Table as Database_Table
import project.Data.Update_Action.Update_Action
import project.Internal.In_Transaction.In_Transaction
import project.Internal.IR.Create_Column_Descriptor.Create_Column_Descriptor
import project.Internal.IR.Query.Query
import project.Internal.IR.SQL_Expression.SQL_Expression
from project.Errors import all
from project.Internal.Result_Set import result_set_to_table

## PRIVATE
   Creates a new database table with the provided structure and returns the name
   of the created table.

   The user-facing function that handles the dry-run logic.
create_table_implementation connection table_name structure primary_key temporary allow_existing on_problems = Panic.recover SQL_Error <|
    connection.base_connection.maybe_run_maintenance
    table_naming_helper = connection.base_connection.table_naming_helper
    table_naming_helper.verify_table_name table_name <| case connection.base_connection.table_exists table_name of
        True ->
            if allow_existing then connection.query (SQL_Query.Table_Name table_name) else Error.throw (Table_Already_Exists.Error table_name)
        False ->
            dry_run = Context.Output.is_enabled.not
            effective_table_name = if dry_run.not then table_name else table_naming_helper.generate_dry_run_table_name table_name
            effective_temporary = temporary || dry_run
            created_table_name = Context.Output.with_enabled <|
                connection.jdbc_connection.run_within_transaction <|
                    if dry_run then
                        ## This temporary table can be safely dropped if it
                           exists, because it only existed if it was created by
                           a previous dry run. `generate_dry_run_table_name`
                           will never return a name of a table that exists but
                           was created outside of a dry run.
                        connection.drop_table table_name if_exists=True
                    internal_create_table_structure connection effective_table_name structure primary_key effective_temporary on_problems
            if dry_run.not then connection.query (SQL_Query.Table_Name created_table_name) else
                created_table = connection.base_connection.internal_allocate_dry_run_table created_table_name
                warning = Dry_Run_Operation.Warning "Only a dry run of `create_table` has occurred, creating a temporary table ("+created_table_name.pretty+") instead of the actual one."
                Warning.attach warning created_table

## PRIVATE
   Assumes the output context is enabled for it to work.
   Does not check if the table already exists - so if it does, it may fail with
   `SQL_Error`. The caller should perform the check for better error handling.
internal_create_table_structure connection table_name structure primary_key temporary on_problems =
    aligned_structure = align_structure connection structure
    resolved_primary_key = resolve_primary_key aligned_structure primary_key
    validate_structure connection.base_connection.column_naming_helper aligned_structure <|
        create_table_statement = prepare_create_table_statement connection table_name aligned_structure resolved_primary_key temporary on_problems
        check_transaction_ddl_support connection
        update_result = create_table_statement.if_not_error <|
            connection.execute_update create_table_statement
        update_result.if_not_error <|
            table_name

## PRIVATE
   A helper that can upload a table from any backend to a database.
   It should be run within a transaction and wrapped in `handle_upload_errors`.
internal_upload_table source_table connection table_name primary_key temporary on_problems=Problem_Behavior.Report_Error row_limit=Nothing =
    case source_table of
        _ : In_Memory_Table ->
            internal_upload_in_memory_table source_table connection table_name primary_key temporary on_problems row_limit
        _ : Database_Table ->
            internal_upload_database_table source_table connection table_name primary_key temporary on_problems row_limit
        _ ->
            Panic.throw <| Illegal_Argument.Error ("Unsupported table type: " + Meta.get_qualified_type_name source_table)

## PRIVATE
select_into_table_implementation source_table connection table_name primary_key temporary on_problems =
    connection.base_connection.maybe_run_maintenance
    table_naming_helper = connection.base_connection.table_naming_helper
    table_naming_helper.verify_table_name table_name <|
        real_target_already_exists = connection.base_connection.table_exists table_name
        if real_target_already_exists then Error.throw (Table_Already_Exists.Error table_name) else
            Panic.recover SQL_Error <| handle_upload_errors <|
                dry_run = Context.Output.is_enabled.not
                connection.jdbc_connection.run_within_transaction <| case dry_run of
                    False ->
                        internal_upload_table source_table connection table_name primary_key temporary on_problems=on_problems row_limit=Nothing
                    True ->
                        tmp_table_name = table_naming_helper.generate_dry_run_table_name table_name
                        table = Context.Output.with_enabled <|
                            ## This temporary table can be safely dropped if it
                               exists, because it only existed if it was created by
                               a previous dry run. `generate_dry_run_table_name`
                               will never return a name of a table that exists but
                               was created outside of a dry run.
                            connection.drop_table tmp_table_name if_exists=True
                            internal_upload_table source_table connection tmp_table_name primary_key temporary=True on_problems=on_problems row_limit=dry_run_row_limit
                        temporary_table = connection.base_connection.internal_allocate_dry_run_table table.name
                        warning = Dry_Run_Operation.Warning "Only a dry run of `select_into_database_table` was performed - a temporary table ("+tmp_table_name+") was created, containing a sample of the data."
                        Warning.attach warning temporary_table

## PRIVATE
   It should be run within a transaction and wrapped in `handle_upload_errors`.
internal_upload_in_memory_table source_table connection table_name primary_key temporary on_problems row_limit =
    In_Transaction.ensure_in_transaction <|
        created_table_name = internal_create_table_structure connection table_name structure=source_table primary_key=primary_key temporary=temporary on_problems=on_problems
        column_names = source_table.column_names

        ## `created_table_name.if_not_error` is used to ensure that if there are
           any dataflow errors up to this point, we want to propagate them and not
           continue. Otherwise, they could 'leak' to `Panic.rethrow` and be wrongly
           raised as panics.
        upload_status = created_table_name.if_not_error <|
            internal_translate_known_upload_errors source_table connection primary_key <|
                insert_template = make_batched_insert_template connection created_table_name column_names
                statement_setter = connection.dialect.get_statement_setter
                Panic.rethrow <|
                    connection.jdbc_connection.batch_insert insert_template statement_setter source_table batch_size=default_batch_size row_limit=row_limit

        upload_status.if_not_error <|
            connection.query (SQL_Query.Table_Name created_table_name)

## PRIVATE
   It should be run within a transaction and wrapped in `handle_upload_errors`.
internal_upload_database_table source_table connection table_name primary_key temporary on_problems row_limit =
    In_Transaction.ensure_in_transaction <|
        connection_check = if source_table.connection.jdbc_connection == connection.jdbc_connection then True else
            Error.throw (Unsupported_Database_Operation.Error "The Database table to be uploaded must be coming from the same connection as the connection on which the new table is being created. Cross-connection uploads are currently not supported. To work around this, you can first `.read` the table into memory and then upload it from memory to a different connection.")

        connection_check.if_not_error <|
            # Warning: in some DBs, calling a DDL query in a transaction may commit it. We may have to have some special handling for this.
            created_table_name = internal_create_table_structure connection table_name structure=source_table primary_key=primary_key temporary=temporary on_problems=on_problems
            upload_status = created_table_name.if_not_error <|
                internal_translate_known_upload_errors source_table connection primary_key <|
                    effective_source_table = case row_limit of
                        Nothing -> source_table
                        _ : Integer -> source_table.limit row_limit
                    ## We need to ensure that the columns in this statement are
                       matching positionally the columns in the newly created
                       table. But we create both from the same source table, so
                       that is guaranteed.
                    copy_into_statement = connection.dialect.generate_sql <|
                        Query.Insert_From_Select created_table_name effective_source_table.column_names effective_source_table.to_select_query
                    Panic.rethrow <| connection.execute_update copy_into_statement

            upload_status.if_not_error <|
                connection.query (SQL_Query.Table_Name created_table_name)

## PRIVATE
   Ensures that provided primary key columns are present in the table and that
   there are no duplicates.
resolve_primary_key structure primary_key = case primary_key of
    Nothing -> Nothing
    _ : Vector -> if primary_key.is_empty then Nothing else
        validated = primary_key.map key->
            if key.is_a Text then key else
                Error.throw (Illegal_Argument.Error ("Primary key must be a vector of column names, instead got a " + (Meta.type_of key . to_display_text)))
        validated.if_not_error <|
            column_names = Set.from_vector (structure.map .name)
            missing_columns = (Set.from_vector primary_key).difference column_names
            if missing_columns.not_empty then Error.throw (Missing_Input_Columns.Error missing_columns.to_vector) else
                primary_key

## PRIVATE
   Inspects any `SQL_Error` thrown and replaces it with an error recipe, that is
   converted into a proper error in an outer layer.

   The special handling is needed, because computing the
   `Non_Unique_Primary_Key` error may need to perform a SQL query that must be
   run outside of the just-failed transaction.
internal_translate_known_upload_errors source_table connection primary_key ~action =
    handler caught_panic =
        error_mapper = connection.dialect.get_error_mapper
        sql_error = caught_panic.payload
        case error_mapper.is_primary_key_violation sql_error of
            True -> Panic.throw (Non_Unique_Primary_Key_Recipe.Recipe source_table primary_key caught_panic)
            False -> Panic.throw caught_panic
    Panic.catch SQL_Error action handler

## PRIVATE
handle_upload_errors ~action =
    Panic.catch Non_Unique_Primary_Key_Recipe action caught_panic->
        recipe = caught_panic.payload
        raise_duplicated_primary_key_error recipe.source_table recipe.primary_key recipe.original_panic

## PRIVATE
type Non_Unique_Primary_Key_Recipe
    ## PRIVATE
    Recipe source_table primary_key original_panic

## PRIVATE
   Creates a `Non_Unique_Primary_Key` error containing information about an
   example group violating the uniqueness constraint.
raise_duplicated_primary_key_error source_table primary_key original_panic =
    agg = source_table.aggregate [Aggregate_Column.Count]+(primary_key.map Aggregate_Column.Group_By)
    filtered = agg.filter column=0 (Filter_Condition.Greater than=1)
    materialized = filtered.read max_rows=1
    case materialized.row_count == 0 of
        ## If we couldn't find a duplicated key, we give up the translation and
           rethrow the original panic containing the SQL error. This could
           happen if the constraint violation is on some non-trivial key, like
           case insensitive.
        True -> Panic.throw original_panic
        False ->
            row = materialized.first_row.to_vector
            example_count = row.first
            example_entry = row.drop 1
            Error.throw (Non_Unique_Primary_Key.Error primary_key example_entry example_count)

## PRIVATE
align_structure : Connection | Any -> Database_Table | In_Memory_Table | Vector Column_Description -> Vector Column_Description
align_structure connection table_or_columns = case table_or_columns of
    vector : Vector         -> align_vector_structure vector
    table : Database_Table  -> structure_from_existing_table connection table
    table : In_Memory_Table -> structure_from_existing_table connection table

## PRIVATE
align_vector_structure vector =
    if vector.is_empty then Error.throw (Illegal_Argument.Error "A table with no columns cannot be created. The `structure` must consist of at list one column description.") else
        vector.map def-> case def of
            _ : Column_Description -> def
            _ : Function ->
                Error.throw (Illegal_Argument.Error "The structure should be a vector of Column_Description. Maybe some arguments of Column_Description are missing?")
            _ ->
                Error.throw (Illegal_Argument.Error "The structure must be an existing Table or vector of Column_Description.")

## PRIVATE
structure_from_existing_table connection table =
    table.columns.map column->
        value_type = connection.dialect.value_type_for_upload_of_existing_column column
        Column_Description.Value column.name value_type

## PRIVATE
   Verifies that the provided structure is valid, and runs the provided action
   or raises an error.

   In particular it checks if there are no clashing column names.
validate_structure column_naming_helper structure ~action =
    column_names = structure.map .name
    # We first check if the names are valid, to throw a more specific error.
    column_naming_helper.validate_many_column_names column_names <|
        problem_builder = Problem_Builder.new
        ## Then we run the deduplication logic. We discard the results, because
           if anything is wrong we will fail anyway.
        unique = column_naming_helper.create_unique_name_strategy
        column_names.each unique.make_unique
        problem_builder.report_unique_name_strategy unique
        problem_builder.attach_problems_before Problem_Behavior.Report_Error <|
            action

## PRIVATE
   Returns the name of the first column in the provided table structure.
   It also verifies that the structure is correct.
   Used to provide the default value for `primary_key` in `create_table`.
first_column_name_in_structure structure = case structure of
    vector : Vector -> align_vector_structure vector . first . name
    table : Database_Table  -> table.column_names.first
    table : In_Memory_Table -> table.column_names.first

## PRIVATE
   Creates a statement that will create a table with structure determined by the
   provided columns.

   The `primary_key` columns must be present in `columns`, but it is the
   responsibility of the caller to ensure that, otherwise the generated
   statement will be invalid.
prepare_create_table_statement : Connection -> Text -> Vector Column_Description -> Vector Text -> Boolean -> Problem_Behavior -> SQL_Statement
prepare_create_table_statement connection table_name columns primary_key temporary on_problems =
    type_mapping = connection.dialect.get_type_mapping
    column_descriptors = columns.map def->
        sql_type = type_mapping.value_type_to_sql def.value_type on_problems
        sql_type_text = type_mapping.sql_type_to_text sql_type
        Create_Column_Descriptor.Value def.name sql_type_text def.constraints
    connection.dialect.generate_sql <|
        Query.Create_Table table_name column_descriptors primary_key temporary

## PRIVATE
   The recommended batch size seems to be between 50 and 100.
   See: https://docs.oracle.com/cd/E18283_01/java.112/e16548/oraperf.htm#:~:text=batch%20sizes%20in%20the%20general%20range%20of%2050%20to%20100
default_batch_size = 100

## PRIVATE
make_batched_insert_template : Connection -> Text -> Vector (Vector Text) -> SQL_Query
make_batched_insert_template connection table_name column_names =
    # We add Nothing as placeholders, they will be replaced with the actual values later.
    pairs = column_names.map name->[name, SQL_Expression.Constant Nothing]
    query = connection.dialect.generate_sql <| Query.Insert table_name pairs
    template = query.prepare.first
    template

## PRIVATE
common_update_table (source_table : Database_Table | In_Memory_Table) (target_table : Database_Table) update_action key_columns error_on_missing_columns on_problems =
    check_target_table_for_update target_table <|
        connection = target_table.connection
        Panic.recover SQL_Error <| handle_upload_errors <|
            connection.jdbc_connection.run_within_transaction <|
                effective_key_columns = if key_columns.is_nothing then [] else key_columns
                check_update_arguments_structure_match source_table target_table effective_key_columns update_action error_on_missing_columns on_problems <|
                    tmp_table_name = connection.base_connection.table_naming_helper.generate_random_table_name "enso-temp-source-table-"
                    dry_run = Context.Output.is_enabled.not
                    row_limit = if dry_run then dry_run_row_limit else Nothing
                    check_for_null_keys_if_any_keys_set source_table effective_key_columns <| Context.Output.with_enabled <|
                        tmp_table = internal_upload_table source_table connection tmp_table_name primary_key=effective_key_columns temporary=True on_problems=Problem_Behavior.Report_Error row_limit=row_limit
                        tmp_table.if_not_error <|
                            resulting_table = append_to_existing_table tmp_table target_table update_action effective_key_columns dry_run=dry_run
                            ## We don't need to drop the table if append panics, because
                               all of this happens within a transaction, so in case the
                               above fails, the whole transaction will be rolled back.
                            connection.drop_table tmp_table.name
                            if dry_run.not then resulting_table else
                                warning = Dry_Run_Operation.Warning "Only a dry run of `update_rows` was performed - the target table has been returned unchanged."
                                Warning.attach warning resulting_table

## PRIVATE
check_target_table_for_update target_table ~action = case target_table of
    _ : In_Memory_Table -> Error.throw (Illegal_Argument.Error "The target table must be a Database table.")
    _ : Database_Table -> if target_table.is_trivial_query . not then Error.throw (Illegal_Argument.Error "The target table must be a simple table reference, like returned by `Connection.query`, without any changes like joins, aggregations or even column modifications.") else
        action

## PRIVATE
   Assumes that `source_table` is a simple table query without any filters,
   joins and other composite operations - if a complex query is needed, it
   should be first materialized into a temporary table.

   If `dry_run` is set to True, only the checks are performed, but the
   operations actually modifying the target table are not.
append_to_existing_table source_table target_table update_action key_columns dry_run = In_Transaction.ensure_in_transaction <|
    helper = Append_Helper.Context source_table target_table key_columns dry_run
    upload_status = case update_action of
        Update_Action.Insert ->
            helper.check_already_existing_rows <|
                helper.insert_rows source_table
        Update_Action.Update ->
            helper.check_rows_unmatched_in_target <|
                helper.check_multiple_target_rows_match <|
                    helper.update_common_rows
        Update_Action.Update_Or_Insert ->
            helper.check_multiple_target_rows_match <|
                helper.update_common_rows . if_not_error <|
                    helper.insert_rows helper.new_source_rows
        Update_Action.Align_Records ->
            helper.check_multiple_target_rows_match <|
                helper.update_common_rows . if_not_error <|
                    helper.insert_rows helper.new_source_rows . if_not_error <|
                        helper.delete_unmatched_target_rows
    upload_status.if_not_error target_table

## PRIVATE
type Append_Helper
    ## PRIVATE
    Context source_table target_table key_columns dry_run

    ## PRIVATE
    connection self = self.target_table.connection

    ## PRIVATE
       Runs the action only if running in normal mode.
       In dry run mode, it will just return `Nothing`.
    if_not_dry_run self ~action = if self.dry_run then Nothing else action

    ## PRIVATE
       The update only affects matched rows, unmatched rows are ignored.
    update_common_rows self = self.if_not_dry_run <|
        update_statement = self.connection.dialect.generate_sql <|
            Query.Update_From_Table self.target_table.name self.source_table.name self.source_table.column_names self.key_columns
        Panic.rethrow <| self.connection.execute_update update_statement

    ## PRIVATE
       Inserts all rows from the source.

       Behaviour is ill-defined if any of the rows already exist in the target.
       If only new rows are supposed to be inserted, they have to be filtered
       before inserting.
    insert_rows self table_to_insert = self.if_not_dry_run <|
        insert_statement = self.connection.dialect.generate_sql <|
            Query.Insert_From_Select self.target_table.name table_to_insert.column_names table_to_insert.to_select_query
        Panic.rethrow <| self.connection.execute_update insert_statement

    ## PRIVATE
       Deletes rows from target table that were not present in the source.
    delete_unmatched_target_rows self = self.if_not_dry_run <|
        delete_statement = self.connection.dialect.generate_sql <|
            Query.Delete_Unmatched_Rows self.target_table.name self.source_table.name self.key_columns
        Panic.rethrow <| self.connection.execute_update delete_statement

    ## PRIVATE
       Finds rows that are present in the source but not in the target.
    new_source_rows self =
        self.source_table.join self.target_table on=self.key_columns join_kind=Join_Kind.Left_Exclusive

    ## PRIVATE
       Checks if any rows from the source table already exist in the target, and
       if they do - raises an error.

       Does nothing if `key_columns` is empty, as then there is no notion of
       'matching' rows.
    check_already_existing_rows self ~continuation =
        if self.key_columns.is_empty then continuation else
            joined = self.source_table.join self.target_table on=self.key_columns join_kind=Join_Kind.Inner
            count = joined.row_count
            if count == 0 then continuation else
                Error.throw (Rows_Already_Present.Error count)

    ## PRIVATE
    check_rows_unmatched_in_target self ~continuation =
        # assert key_columns.not_empty
        unmatched_rows = self.new_source_rows
        count = unmatched_rows.row_count
        if count != 0 then Error.throw (Unmatched_Rows.Error count) else continuation

    ## PRIVATE
       Check if there are rows in source that match multiple rows in the target.
    check_multiple_target_rows_match self ~continuation =
        ## This aggregation will only find duplicated in target, not in the source,
           because the source is already guaranteed to be unique - that was checked
           when uploading the temporary table with the key as its primary key.
        check_multiple_rows_match self.target_table self.source_table self.key_columns <|
            continuation

## PRIVATE
   This helper ensures that all arguments are valid.

   The `action` is run only if the input invariants are satisfied:
   - all columns in `source_table` have a corresponding column in `target_table`
     (with the same name),
   - all `key_columns` are present in both source and target tables.
check_update_arguments_structure_match source_table target_table key_columns update_action error_on_missing_columns on_problems ~action =
    check_source_column source_column =
        # The column must exist because it was verified earlier.
        target_column = target_table.get source_column.name
        source_type = source_column.value_type
        target_type = target_column.value_type
        if source_type == target_type then [] else
            if source_type.can_be_widened_to target_type then [Inexact_Type_Coercion.Warning source_type target_type] else
                Error.throw (Column_Type_Mismatch.Error source_column.name target_type source_type)

    source_columns = Set.from_vector source_table.column_names
    target_columns = Set.from_vector target_table.column_names
    extra_columns = source_columns.difference target_columns
    if extra_columns.not_empty then Error.throw (Unmatched_Columns.Error extra_columns.to_vector) else
        missing_columns = target_columns.difference source_columns
        if missing_columns.not_empty && error_on_missing_columns then Error.throw (Missing_Input_Columns.Error missing_columns.to_vector "the source table") else
            key_set = Set.from_vector key_columns
            missing_source_key_columns = key_set.difference source_columns
            missing_target_key_columns = key_set.difference target_columns
            if missing_source_key_columns.not_empty then Error.throw (Missing_Input_Columns.Error missing_source_key_columns.to_vector "the source table") else
                if missing_target_key_columns.not_empty then Error.throw (Missing_Input_Columns.Error missing_target_key_columns.to_vector "the target table") else
                    if (update_action != Update_Action.Insert) && key_columns.is_empty then Error.throw (Illegal_Argument.Error "For the `update_action = "+update_action.to_text+"`, the `key_columns` must be specified to define how to match the records.") else
                        # Verify type matching
                        problems = source_table.columns.flat_map check_source_column
                        problems.if_not_error <|
                            on_problems.attach_problems_before problems action

## PRIVATE
default_key_columns (table : Database_Table | In_Memory_Table) =
    check_target_table_for_update table <|
        table.get_primary_key

## PRIVATE
   A variant of `default_key_columns` that will raise an error if no key columns
   were found.
default_key_columns_required table =
    key_columns = default_key_columns table
    ok = key_columns.is_nothing.not && key_columns.not_empty
    if ok then key_columns else
        Error.throw (Illegal_Argument.Error "No primary key found to serve as a default value for `key_columns`. Please set the argument explicitly.")

## PRIVATE
dry_run_row_limit = 1000

## PRIVATE
   Verifies that the used driver supports transactional DDL statements.

   Currently, all our drivers should support them. This check is added, so that
   when we are adding a new drivers, we don't forget to check if it supports
   transactional DDL statements - if it does not - we will need to add some
   additional logic to our code.

   It is a panic, because it is never expected to happen in user code - if it
   happens, it is a bug in our code.
check_transaction_ddl_support connection =
    supports_ddl = connection.jdbc_connection.with_metadata metadata->
        metadata.supportsDataDefinitionAndDataManipulationTransactions
    if supports_ddl.not then
        Panic.throw (Illegal_State.Error "The connection "+connection.to_text+" does not support transactional DDL statements. Our current implementation of table updates relies on transactional DDL. To support this driver, the logic needs to be amended.")

## PRIVATE
common_delete_rows target_table key_values_to_delete key_columns allow_duplicate_matches =
    check_delete_rows_arguments target_table key_values_to_delete key_columns <|
        connection = target_table.connection
        Panic.recover SQL_Error <| connection.jdbc_connection.run_within_transaction <|
            dry_run = Context.Output.is_enabled.not
            check_for_null_keys key_values_to_delete key_columns <| Context.Output.with_enabled <|
                source = prepare_source_table_for_delete_matching key_values_to_delete connection key_columns dry_run
                source_db_table = source.db_table
                source_db_table.if_not_error <|
                    check_duplicate_key_matches_for_delete target_table source_db_table key_columns allow_duplicate_matches <|
                        affected_row_count = case dry_run of
                            True ->
                                # On dry run, we just compute the row count by joining.
                                target_table.join source_db_table on=key_columns join_kind=Join_Kind.Inner . row_count
                            False ->
                                # On a real run, we get the row count from the DELETE statement return value.
                                if source.tmp_table_name.is_nothing then
                                    Panic.throw (Illegal_State.Error "Impossible. This is a bug in the Database library.")
                                delete_statement = connection.dialect.generate_sql <|
                                    Query.Delete_Matching_Rows target_table.name source.tmp_table_name key_columns
                                connection.execute_update delete_statement

                        ## We don't need to drop the table if update panics, because
                           all of this happens within a transaction, so in case the
                           above fails, the whole transaction will be rolled back.
                        source.drop_temporary_table connection
                        if dry_run.not then affected_row_count else
                            suffix = source.dry_run_message_suffix
                            warning = Dry_Run_Operation.Warning "Only a dry run of `delete_rows` was performed - the target table has not been changed."+suffix
                            Warning.attach warning affected_row_count

## PRIVATE
   Prepares the source table for the delete operation.

   It selects only the key columns and ensures keys are distinct to avoid primary key clashes.
   In dry-run mode, if the table is an SQL query, it is kept as is to avoid copying data.
   In real mode, the table is copied into a temporary table to simplify the DELETE FROM query.
   If the source table is in-memory, it is uploaded in either mode, but in dry-run mode only a sample of the rows is processed.
prepare_source_table_for_delete_matching (key_values_to_delete : Database_Table | In_Memory_Table) connection key_columns dry_run =
    ## We select only the key columns and discard anything else.
       We also call distinct to ensure that we will not have primary-key
       duplicate issues when uploading the temporary table.
    prepared_table = key_values_to_delete.select_columns key_columns . distinct
    case dry_run of
        True ->
            case prepared_table of
                _ : Database_Table ->
                    Delete_Rows_Source.Value prepared_table Nothing
                _ : In_Memory_Table ->
                    tmp_table_name = connection.base_connection.table_naming_helper.generate_random_table_name "enso-temp-keys-table-"
                    uploaded_table = internal_upload_in_memory_table prepared_table connection tmp_table_name primary_key=key_columns temporary=True on_problems=Problem_Behavior.Report_Error row_limit=dry_run_row_limit

                    row_limit_exceeded = prepared_table.row_count > dry_run_row_limit
                    dry_run_message_suffix = case row_limit_exceeded of
                        False -> ""
                        True  -> " (Only the first "+dry_run_row_limit.to_text+" distinct rows out of "+prepared_table.row_count.to_text+" were used for the dry run. The count rows affected by the actual operation may be larger once it is run with Output context enabled.)"
                    Delete_Rows_Source.Value uploaded_table tmp_table_name dry_run_message_suffix=dry_run_message_suffix

        False ->
            tmp_table_name = connection.base_connection.table_naming_helper.generate_random_table_name "enso-temp-keys-table-"
            copied_table = internal_upload_table prepared_table connection tmp_table_name primary_key=key_columns temporary=True on_problems=Problem_Behavior.Report_Error row_limit=Nothing
            Delete_Rows_Source.Value copied_table tmp_table_name

## PRIVATE
type Delete_Rows_Source
    ## PRIVATE
    Value (db_table : Database_Table) (tmp_table_name : Nothing | Text) (dry_run_message_suffix : Text = "")

    ## PRIVATE
       Drops a temporarily created table, if there was one.
    drop_temporary_table self connection =
        if self.tmp_table_name.is_nothing.not then
            connection.drop_table self.tmp_table_name


## PRIVATE
check_delete_rows_arguments target_table key_values_to_delete key_columns ~continuation =
    check_target_table_for_update target_table <|
        if key_columns.is_empty then Error.throw (Illegal_Argument.Error "One or more key columns must be provided to correlate the rows to be deleted.") else
            key_set = Set.from_vector key_columns
            missing_target_key_columns = key_set . difference (Set.from_vector target_table.column_names)
            if missing_target_key_columns.not_empty then Error.throw (Missing_Input_Columns.Error missing_target_key_columns.to_vector "the target table") else
                missing_source_key_columns = key_set . difference (Set.from_vector key_values_to_delete.column_names)
                if missing_source_key_columns.not_empty then Error.throw (Missing_Input_Columns.Error missing_source_key_columns.to_vector "the key values to delete table") else
                    continuation

## PRIVATE
check_duplicate_key_matches_for_delete target_table tmp_table key_columns allow_duplicate_matches ~continuation =
    if allow_duplicate_matches then continuation else
        check_multiple_rows_match target_table tmp_table key_columns <|
            continuation

## PRIVATE
   Checks if any rows identified by `key_columns` have more than one match between two tables.
check_multiple_rows_match left_table right_table key_columns ~continuation =
    joined = left_table.join right_table on=key_columns join_kind=Join_Kind.Inner
    counted = joined.aggregate [Aggregate_Column.Count]+(key_columns.map (Aggregate_Column.Group_By _))
    duplicates = counted.filter 0 (Filter_Condition.Greater than=1)
    example = duplicates.read max_rows=1
    case example.row_count == 0 of
        True -> continuation
        False ->
            row = example.first_row . to_vector
            offending_key = row.drop 1
            count = row.first
            Error.throw (Multiple_Target_Rows_Matched_For_Update.Error offending_key count)

## PRIVATE
check_for_null_keys table key_columns ~continuation =
    keys = table.select_columns key_columns
    is_any_key_blank = keys.columns.map (_.is_nothing) . reduce (||)
    null_keys = table.filter is_any_key_blank Filter_Condition.Is_True
    example = null_keys.read max_rows=1
    case example.row_count == 0 of
        True -> continuation
        False ->
            example_key = example.first_row.to_vector
            Error.throw (Null_Values_In_Key_Columns.Error example_key)

## PRIVATE
check_for_null_keys_if_any_keys_set table key_columns ~continuation =
    if key_columns.is_empty then continuation else
        check_for_null_keys table key_columns continuation
